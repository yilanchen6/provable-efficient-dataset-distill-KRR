
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="bootstrap.js"></script>
<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style type="text/css">
body {
    font-family: "Arial", "calibri", "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica,  "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}

h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

img {
  max-width: 100%;
  height: auto;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
}
.maintext {
    font-size: 17px;
}

video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.example-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #4cd468;
  color: white !important;
  font-size: 20px;
  width: 220px;
  font-weight: 600;
}
.example-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.example-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}



/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}








.rounded-circle {
  border-radius: 50% !important;
}






/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title>Provable and Efficient Dataset Distillation for Kernel Ridge Regression</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="The Importance of Prompt Tuning for Automated Neuron Explanations"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
    </head>

 <body>


<div class="container">
    <div class="paper-title">
    <h1> 
        Provable and Efficient Dataset Distillation <br>
        for Kernel Ridge Regression
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://yilanchen6.github.io/">Yilan Chen<sup>1</sup></a>,
                <a href="https://weihuang05.github.io/">Wei Huang<sup>2</sup></a>,
                <a href="https://lilywenglab.github.io/">Tsui-Wei (Lily) Weng<sup>1</sup></a>
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span> <sup>1</sup>UCSD</span>
            <span> <sup>2</sup>RIEKN AIP</span>
        </div>


        <div class="affil-row">
            <div class="venue text-center"><b>NeurIPS 2024</b></div>
        </div>

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://openreview.net/pdf?id=WI2VpcBdnd">
                <span class="material-icons"> description </span> 
                <div class="maintext">Paper</div> 
            </a>
            <div class="paper-btn-coming-soon">
                <a class="paper-btn" href="https://github.com/Trustworthy-ML-Lab/provable-efficient-dataset-distill-KRR">
                    <span class="material-icons"> code </span>
                    <div class="maintext"> Code </div>
                </a>
            </div>
        </div></div>
    </div>
    

    
    <section id="abstract">
        <hr>
        <h2>Abstract</h2>
        <div class="flex-row">
            <p class="maintext">
                Deep learning models are now trained on increasingly larger datasets, making it
                crucial to reduce computational costs and improve data quality. Dataset distillation
                aims to distill a large dataset into a small synthesized dataset such that models
                trained on it can achieve similar performance to those trained on the original
                dataset. While there have been many empirical efforts to improve dataset distillation
                algorithms, a thorough theoretical analysis and provable, efficient algorithms are
                still lacking. </p>
            <p class="maintext">
                In this paper, by focusing on dataset distillation for kernel ridge
                regression (KRR), we show that one data point per class is already necessary and
                sufficient to recover the original model’s performance in many settings. 
                <ul>
                    <li>For linear ridge regression and KRR with surjective feature mappings, we provide necessary
                        and sufficient conditions for the distilled dataset to recover the original model’s
                        parameters.</li>
                    <li>For KRR with injective feature mappings of deep neural networks, we
                        show that while one data point per class is not sufficient in general, k+1 data points
                        can be sufficient for deep linear neural networks, where k is the number of classes.</li>
                    <li>Our theoretical results enable directly constructing analytical solutions for distilled
                        datasets, resulting in a provable and efficient dataset distillation algorithm for KRR.
                        We verify our theory experimentally and show that our algorithm outperforms
                        previous work such as KIP while being significantly more efficient, e.g. 15840×
                        faster on CIFAR-100.</li>
                  </ul>
            </p>
        </div>
    </section>
    <!-- <section id="Overview">
        <div class="mx-auto">
            <center><img class="card-img-top" src="assets/overview.gif" style="max-width: none;"></center>
            <center>
                <p class="caption">
                    Overview of our paper.
                </p>
            </center>
        </div>
    </section> -->
    <section id="What is Dataset Distillation">
        <hr>
        <h2>What is Dataset Distillation</h2>
            
            <div class="mx-auto">
                <center><img class="card-img-top" style="max-width: 95%;height: auto;" src="assets/data_distill.png"></center>
                <center><p class="caption">An illustration of dataset distillation from <a href="#Ref1">[1]</a></p></center>
            </div>
            <div class="flex-row">
                <div class="mx-auto">
                    <p class="maintext">
                    Given an original dataset: \((X, Y) \in \mathbb{R}^{d \times n} \times \mathbb{R}^{k \times n}\), where \(d\) is the dimenion of the data, 
                    \(n\) is the number of the original data, and \(k\) is the dimension of the label, dataset distillation aims to distill the large original dataset 
                    into a small synthesized dataset \((X_S, Y_S) \in \mathbb{R}^{d \times m} \times \mathbb{R}^{k \times n} \), such that models trained on it 
                    can achieve similar performance to those trained on the original dataset, where \(m\) is the number of distilled data and \(m \ll n\).
                    </p>
                    
                </div>
            </div>
    </section>

    <section  id=" Dataset Distillation for Kernel Ridge Regression (KRR)">
        <hr>
        <h2> Dataset Distillation for Kernel Ridge Regression (KRR)</h2>
        <p class="maintext">
        A Kernel ridge regression (KRR) is \(f(x) = W \phi(x)\), where \(\phi: \mathbb{R}^{d} \mapsto \mathbb{R}^{p}\) and \(W \in \mathbb{R}^{k \times p}\), 
        that minimize the following loss:
        $$\min_{W} \|Y - W \phi(X)\|_F^2 + \lambda \|W\|^2$$
        where \(\lambda > 0\) is the regularization parameter.
        The solution can be computed analytically as \(W = Y \phi_{\lambda}(X)^+\), where 
        $$\phi_{\lambda}(X)^+ = \left\{  
            \begin{array}{ll}  
            (K(X, X) + \lambda I_n)^{-1} \phi(X)^\top = \phi(X)^\top (\phi(X) \phi(X)^\top + \lambda I_p)^{-1}, &\text{if  $\lambda >0$, }  \\  
            \phi(X)^+, &\text{if  $\lambda =0$.}
            \end{array}  \right. $$ 
        and \(K(X, X) = \phi(X)^\top \phi(X) \in \mathbb{R}^{n \times n}\). \(\phi_{\lambda}(X)\) can be considered as regularized features.
        Linear ridge regression is a special case of kernel ridge regression with \(\phi(X) = X \). 
        <br><br>
        Similarly, a KRR trained on distilled dataset with regularization \(\lambda_S \geq 0\) is \(f_S(x) = W_S \phi(x)\),
        The goal of dataset distillation is to find \((X_S, Y_S)\) such that \(W_S = W\). 
        
        </p>

    </section>
    <section>
        <hr>
        <h2>Dataset Distillation for Linear Ridge Regression (LRR)</h2>
        <p class="maintext">    
            For a LRR model, we show that \(k\) distilled data points (one per class) are necessary and 
            sufficient to guarantee \(W_S = W\). We provide analytical solutions of such \((X_S, Y_S)\) allowing us to
            compute the distilled dataset analytically instead of having to learn it heuristically in existing works.<br>
        </p>
        <div class="mx-auto">
            <center><img class="card-img-top" style="max-width: 90%;height: auto;" src="assets/dd_linear.png"></center>
            <!-- <center><p class="caption">Analytical Computation for Linear Ridge Regression.</p></center> -->
        </div>
        <br>

        <p class="maintext">
            Intuitively, original dataset \((X, Y)\) is compressed into \((X_S, Y_S)\) through original model’s parameter \(W\).
            <ul>    
                <li>When \(m=k\), there is only one solution. When \(𝜆_𝑆=0, X_S=(Y_S^+ W)^+ \).
                <li>When \(m>k\), there are infinitely many distilled datasets since \(Z\) is a free variable to choose
                <li>When \(m=n\), \((X, Y)\) is a distilled dataset for itself.
                <li>When \(m>n\), we can generate more data than original dataset.
            </ul>
            
            Below is an illustration of distilled data of MNIST and CIFAR-100 when \(m=k\).
        </p>
        <div class="mx-auto">
            <center><img class="card-img-top" style="max-width: 90%;height: auto;" src="assets/dd_one_data_per_class.png"></center>
            <!-- <center><p class="caption">Distilled data of MNIST (first row) and CIFAR-100 (second row) for LRR when \(m=k\).</p></center> -->
        </div>

        <p class="maintext">    
        Besides, we can also 
        <ol>    
            <li>Find realistic distilled data that is close to original data by solving closed-from solution.
            <li>Generate distilled data  from random noise.
        </ol>
        Below is an illustration on MNIST and CIFAR-100.
        </p>
        <div class="mx-auto">
            <center><img class="card-img-top" style="max-width: 90%;height: auto;" src="assets/dd_real.png"></center>
            <!-- <center><p class="caption">Initialized data (first row), distilled data generated from real images using techniques in -->
                <!-- Sec 4.2 (second row), and distilled data generated from random noise using techniques in Sec 4.1
                (third row) for a LRR with m = 500 on MNIST and CIFAR-100. IPC: images per clas.</p></center> -->
        </div>

    </section>
    <section>
        <hr>
        <h2>Kernel Ridge Regression (KRR)</h2>
            <h3>Surjective Feature Mapping</h3>
            <p class="maintext">
                The results of LRR can be extended to KRR by replacing \(X_S\) with \(\phi(X_S) \).
                When \(\phi\) is surjective or bijective, we can always find a \(X_S\) for a desired \(\phi(X_S) \).

                Examples of surjective \(\phi: p \leq d\):
                <ol>    
                    <li>Invertible NNs
                    <li>Fully-connected NN (FCNN)
                    <li>Convolutional Neural Network (CNN)
                    <li>Random Fourier Features (RFF)
                </ol>
            

            <h3>Non-surjective Feature Mapping</h3>
            <p class="maintext">
            For non-surjective \(\phi\) such as deep nonlinear NNs, one data per class is generally not sufficient as long as \((Y_S^+ W)^+\) is not in the range space of \(\phi\).
            For deep linear NNs, we show \(m=k+1\) can be sufficient under certain conditions.
            </p>
            
            
            


        </p>
    </section>


    <section>
        <hr>
        <h2>Comparison with Existing Theoretical Results</h2>
        <p class="maintext">
            Below is a comparison with existing theoretical analysis of dataset distillation. 
        </p>
        <div class="mx-auto">
            <center><img class="card-img-top" style="max-width: 95%;height: auto;" src="assets/compare.png"></center>
            <!-- <center><p class="caption">An illustration of dataset distillation from <a href="#Ref1">[1]</a></p></center> -->
        </div>

        <h2>Experiments</h2>
        <p class="maintext">
            For surjective 𝜙, our algorithm outperforms previous work such as KIP <a href="#Ref2">[2]</a> while being significantly more efficient.
        </p>
        <div class="mx-auto">
            <center><img class="card-img-top" style="max-width: 90%;height: auto;" src="assets/dd_experiment.png"></center>
            <!-- <center><p class="caption">Average size of prediction sets on CIFAR10 & CIFAR100</p></center> -->
        </div>
        
    </section>


    <section>
        <hr>
        <h2>Related works</h2>
        <p class="maintext">
        [1] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, Alexei A. Efros.
        <a id="Ref1" href="https://arxiv.org/pdf/1811.10959">"Dataset Distillation."</a>
        2020.
        <br>
        [2] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee.
        <a id="Ref2" href="https://arxiv.org/pdf/2107.13034">"Dataset distillation with infinitely wide convolutional networks."</a>
        Advances in Neural Information Processing Systems. 2021.
    </section>
    <section class="maintext">
        <hr>
        <h2>Cite this work</h2>
        Yilan Chen, Wei Huang, Tsui-Wei Weng, <a href="https://openreview.net/pdf?id=WI2VpcBdnd">
            <b>Provable and Efficient Dataset Distillation for Kernel Ridge Regression</b>
        </a>, NeurIPS 2024.
        <pre>
            <code>
@inproceedings{
    chen2024provable,
    title={Provable and Efficient Dataset Distillation for Kernel Ridge Regression},
    author={Yilan Chen and Wei Huang and Tsui-Wei Weng},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=WI2VpcBdnd}
    }
            </code>
        </pre>
    </section>
    <section>
        <hr>
        This webpage template was recycled from <a href='https://nv-tlabs.github.io/LION/'>here</a>.
        <center><p><a href='https://accessibility.ucsd.edu/'><b>Accessibility</b></a></p></center>
    </section>
</div>
</body>
</html>
